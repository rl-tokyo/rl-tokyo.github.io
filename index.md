---
layout: default
---

# 活動内容

1. 火曜勉強会: 隔週火曜に東京駅近郊で勉強会（教科書輪読または論文紹介）
2. アウトプット活動: 勉強会での議論から派生したアウトプット活動（書籍・論文の出版）

1.の火曜勉強会が主な活動内容になります。

## アウトプット活動
勉強会での活動・議論から派生してのアウトプットを最終目標としています。これまでに次のようなアウトプット実績があります。

### 速習 強化学習 ―基礎理論とアルゴリズム―
輪講メンバーによる訳本、「[速習 強化学習 ―基礎理論とアルゴリズム―](http://www.kyoritsu-pub.co.jp/bookdetail/9784320124226)」が2017年9月21日に共立出版から刊行されました。

<img width="30%" src="https://raw.githubusercontent.com/rl-tokyo/szepesvari-book/master/sokusyu-rl.png" />

### 論文
**太字**が勉強会参加者です。

- **S. Koyamada**, **Y. Kikuchi**, A. Kanemura, **S. Maeda**, and S. Ishii: “[Neural sequence model training via α-divergence minimization.](https://arxiv.org/abs/1706.10031)” [LGNL](https://sites.google.com/site/langgen17/), ICML Workshop, 2017.
  - 第56回の論文紹介での議論から派生

## 教科書輪読（水曜勉強会）
"[Algorithms for Reinforcement Learning](https://sites.ualberta.ca/~szepesva/RLBook.html)" の輪講をしていました。3周して終了しました。

- Szepesvári 2010 "[Algorithms for Reinforcement Learning](https://sites.ualberta.ca/~szepesva/RLBook.html)" (Morgan & Claypool)
- 毎回5ページ程度

1. 1周目: 2015/11/20〜
2. 2週目: 2016/04/06〜
3. 3週目: 2017/03/01〜

こちらでの輪講資料をもとに[速習 強化学習 ―基礎理論とアルゴリズム―](http://www.kyoritsu-pub.co.jp/bookdetail/9784320124226)が刊行されました。

## 論文紹介（水曜勉強会）

発表担当者が自分の好みで紹介したい論文を一本紹介してもらいます。
資料の作成は任意になります。
資料の作成よりは論文自体の理解を優先してもらい、
当日の発表はホワイトボード等を使って説明して頂く形でも問題ありません。
1〜2時間かけて一本をじっくり読みます。

# 活動履歴

|   # | 日付 | 担当 | 内容 | 資料 |
|:----|:-----------|:---------------|
|   1 | 2015/10/22 | @sotetsuk / @RodeoBoy24420 |キックオフ: @sotetsukから趣旨の説明と@RodeoBoy24420から強化学習概要 | [pdf](resource/20151022-RodeoBoy24420.pdf) |
|   2 | 2015/11/02 | @smochi |教科書: p.3-p.6 | |
|   3 | 2015/11/12 | @nnnnishi |教科書: p.7-p.12 | |
|   4 | 2015/11/17 | @RodeoBoy24420 |教科書: p.12-p.17 | |
|   5 | 2015/11/24 | @takayukisekine |教科書: p.17-p.23 | |
|   6 | 2015/12/02 | @ikki407 |教科書: p.23-p.29 | [pdf](resource/20151202-ikki407.pdf) |
|   7 | 2015/12/08 | @sotetsuk |教科書: p.29-p.35 | |
|   8 | 2015/12/16 | @fullflu |教科書: p.35-p.40 | [pdf](resource/20151216-fullflu.pdf) |
|   9 | 2016/01/18 | @smochi |教科書: p.40-p.45  | |
|  10 | 2016/01/27 | @nnnnishi |教科書: p.45-p.50  | |
|  11 | 2016/02/08 | @RodeoBoy24420  |教科書: p.50-p.56 | |
|  12 | 2016/02/22 | @takayukisekine |教科書: p.56-p.62 | |
|  13 | 2016/02/29 | @sotetsuk |教科書: p.62-p.67 | |
|  14 | 2016/03/28 | @ikki407  |教科書: p.68-p.74 | |
|  15 | 2016/04/06 | @sotetsuk |教科書: p.3-p.6 | |
|  16 | 2016/04/13 | @muupan |論文: [Mastering the game of Go with deep neural networks and tree search](https://vk.com/doc-44016343_437229031?dl=56ce06e325d42fbc72) | [pdf](resource/20160413-muupan.pdf) |
|  17 | 2016/04/20 | @smochi |教科書: p.7-p.12 | |
|  18 | 2016/04/27 | @takayukisekine |論文: [Deep Reinforcement Learning with Attention for Slate Markov Decision Processes with High-Dimensional States and Actions](https://arxiv.org/abs/1512.01124) | [pdf](resource/20160427-takayukisekine.pdf) |
|  19 | 2016/05/11 | @smochi |教科書: p.7-p.12（続き） | |
|  20 | 2016/05/18 | @RodeoBoy24420 |論文: [Algorithms for Inverse Reinforcement Learning](http://ai.stanford.edu/~ang/papers/icml00-irl.pdf) | [pdf](resource/20160518-RodeoBoy24420.pdf) |
|  21 | 2016/05/25 | @nnnnishi |教科書: p.12-p.17 | |
|  22 | 2016/06/01 | @sotetsuk |論文: [Dopamine Cells Respond to Predicted Events during Classical Conditioning: Evidence for Eligibility Traces in the Reward-Learning Network](http://www.jneurosci.org/content/25/26/6235) | [slideshare](https://www.slideshare.net/sotetsukoyamada/22-62639753) |
|  23 | 2016/06/08 | @YuriCat |教科書: p.17-p.23 | |
|  24 | 2016/06/15 | @ikki407 |論文: [A Contextual-Bandit Approach to Personalized News Article Recommendation](https://arxiv.org/abs/1003.0146) | [pdf](resource/20160615-ikki407.pdf) |
|  25 | 2016/06/22 | @fullflu |教科書: p.23-p.29 | |
|  26 | 2016/06/29 | Ueki-san |論文: [True Online TD(λ) ](http://proceedings.mlr.press/v32/seijen14.pdf)| |
|  27 | 2016/07/06 | @muupan |教科書: p.29-p.35 | |
|  28 | 2016/07/20 | @smochi |論文: [Dynamic pricing policies for interdependent perishable products or services using reinforcement learning](http://dl.acm.org/citation.cfm?id=2953251) | |
|  29 | 2016/08/03 | @takayukisekine |教科書: p.35-p.40 | |
|  30 | 2016/08/10 | @nnnnishi |論文: [Ensemble Contextual Bandits for Personalized Recommendation](http://dl.acm.org/citation.cfm?id=2645732) | [slideshare](https://www.slideshare.net/NaokiNISHIMURA2/30ensemble-contextual-bandits-for-personalized-recommendation) |
|  31 | 2016/08/24 | @RodeoBoy24420 |教科書: p.40-p.45 | |
|  32 | 2016/08/31 | @sotetsuk |論文: [Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation](https://arxiv.org/abs/1604.06057) | [slideshare](https://www.slideshare.net/sotetsukoyamada/kulkarni-et-al-2016) |
|  33 | 2016/09/14 | @ikki407 |教科書: p.45-p.50  | |
|  34 | 2016/09/21 | @YuriCat |論文: [Unifying Count-Based Exploration and Intrinsic Motivation](https://arxiv.org/abs/1606.01868) | [slideshare](https://www.slideshare.net/KatsukiOhto/unifying-count-based-exploration-and-intrinsic-motivation) |
|  35 | 2016/09/28 | @shiba24 |教科書: p.50-p.56 | |
|  36 | 2016/10/05 | @takayukisekine |論文: [SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient](https://arxiv.org/abs/1609.05473) | [slideshare](http://www.slideshare.net/TakayukiSekine1/seq-gan-66772221) |
|  37 | 2016/10/19 | @shiba24 |教科書: p.50-p.56 | |
|  38 | 2016/10/26 | @shiba24 |論文: [Hybrid computing using a neural network with dynamic external memory](https://www.nature.com/nature/journal/v538/n7626/abs/nature20101.html?lang=en) | [pdf](resource/20161026_dnc.pdf) |
|  39 | 2016/11/02 | @muupan |教科書: p.56-p.62 | |
|  40 | 2016/11/09 | @fullflu |論文: [DCM Bandits: Learning to Rank with Multiple Clicks](https://arxiv.org/abs/1602.03146) | [pdf](resource/20161109-fullflu.pdf) |
|  41 | 2016/11/30 | @fullflu |教科書: p.62-p.67 | |
|  43 | 2016/12/07 | @shiba24 |教科書: p.68-p.74 | |
|  44 | 2017/01/11 | @muupan |論文: [Safe and Efficient Off-PolicyReinforcement Learning](https://arxiv.org/abs/1606.02647) | [pdf](resource/20170111-muupan.pdf) |
|  45 | 2017/01/18 | @RodeoBoy24420 | 論文: [Application of fuzzy Q-learning for electricity market modeling by considering renewable power penetration](http://www.sciencedirect.com/science/article/pii/S1364032115014033) | [pdf](resource/20170118-RodeoBoy24420.pdf) |
|  46 | 2017/02/01 | @STRatANG |論文: [Sample Efficient Actor-Critic with Experience Replay](https://arxiv.org/abs/1611.01224) | [pdf](resource/20170201-STRatANG.pdf) |
|  47 | 2017/02/08 | @ororoku |論文: [The Dynamics of Reinforcement Learning in Cooperative Multiagent Systems](https://www.aaai.org/Papers/AAAI/1998/AAAI98-106.pdf) | [pdf](resource/20170208-ororoku.pdf) |
|  48 | 2017/02/15 | @smochi |論文: [Artist Agent: A Reinforcement Learning Approach to Automatic Stroke Generation in Oriental Ink Painting](https://arxiv.org/abs/1206.4634) | [pdf](resource/20170212-smochi.pdf) |
|  49 | 2017/03/01 | @sotetsuk | 教科書: p.3-p.12 | |
|  50 | 2017/03/08 | @nnnnishi |論文: [Optimal Asset Allocation using Adaptive Dynamic Programming](https://papers.nips.cc/paper/1121-optimal-asset-allocation-using-adaptive-dynamic-programming.pdf) / [Enhancing Q-Learning for Optimal Asset Allocation](https://papers.nips.cc/paper/1427-enhancing-q-learning-for-optimal-asset-allocation.pdf) | [pdf](resource/20170308-nnnnishi.pdf) |
|  51 | 2017/03/15 | @STRatANG | 教科書: p.12-p.17 | |
|  52 | 2017/03/22 | @YuriCat |論文: [Combining policy gradient and Q-learning](https://arxiv.org/abs/1611.01626) | [slideshare](https://www.slideshare.net/sotetsukoyamada/pgq-combining-policy-gradient-and-qlearning) |
|  53 | 2017/03/29 | @pacocat | 教科書: p.17-p.23 | |
|  54 | 2017/04/05 | @takayukisekine |論文: [Evolution Strategies as a Scalable Alternative to Reinforcement Learning](https://arxiv.org/abs/1703.03864) | [pdf](resource/20170405-sekine.pdf) |
|  55 | 2017/04/12 | @kiyukuta | 教科書: p.23-p.29 | |
|  56 | 2017/04/19 | @sotetsuk |論文: [Reward Augmented Maximum Likelihood for Neural Structured Prediction](https://arxiv.org/abs/1609.00150) | [slideshare](https://www.slideshare.net/sotetsukoyamada/reward-augmented-maximum-likelihood-for-neural-structured-prediction) |
|  57 | 2017/04/26 | Kume-san | 教科書: p.29-p.35 | |
|  58 | 2017/05/10 | Yamada-san |論文: [Overcoming catastrophic forgetting in neural networks](https://arxiv.org/abs/1612.00796) | [pdf](resource/20170510-yamada.pdf) |
|  59 | 2017/05/17 | @ororoku | 教科書: p.35-p.40 | |
|  60 | 2017/05/24 | @smochi |論文: [Prioritized Experience Replay](https://arxiv.org/abs/1511.05952) | [google slide](https://docs.google.com/presentation/d/1P_ks8cqXcQmc8rBk7QlxcBHwfSdlNYnPmnWF0yj_nYs/edit#slide=id.g22de88c68b_1_160) |
|  61 | 2017/05/31 | @eratostennis | 教科書: p.40-p.45 | |
|  62 | 2017/06/07 | @sotetsuk |論文: [Bridging the Gap Between Value and Policy Based Reinforcement Learning](https://arxiv.org/abs/1702.08892) | [Dropbox paper](https://paper.dropbox.com/doc/Bridging-the-Gap-Between-Value-and-Policy-Based-Reinforcement-Learning-JmxgdnSMzEQUmBTk1C31T) |
|  63 | 2017/06/14 | @STRatANG | 教科書: p.45-p.50 | [当日メモ](https://paper.dropbox.com/doc/20170614-63-CKfWM2TQ0gyQW4aGEypZ0) |
|  64 | 2017/06/21 | @YuriCat | [The Predictron: End-To-End Learning and Planning](https://arxiv.org/abs/1612.08810) | [Dropbox paper](https://paper.dropbox.com/doc/The-Predictron-End-to-End-Learning-and-Planning-B8bAPaUfElK0yhEPP2Qgu) |
|  65 | 2017/06/28 | @fullflu | 教科書: p.50-p.56 | [当日メモ](https://paper.dropbox.com/doc/65-i42HBMyaDfTc3wCEasm1c) |
|  66 | 2017/07/05 | @shiba24 | [Trust Region Policy Optimization](https://arxiv.org/abs/1502.05477) | [google slide](https://docs.google.com/presentation/d/1BUAbeNbO_-cEBpFQgN2mnrSaUeF-QLD9OPIFpai9Gz8/edit) / [当日メモ](https://paper.dropbox.com/doc/66-dRD0JX8C0PTnsMi8E1vaF) |
|  67 | 2017/07/12 | Kume-san | 教科書: p.56-p.62 | [当日メモ](https://paper.dropbox.com/doc/67-d8PCnv47TrSSa1tNqibhN) |
|  68 | 2017/07/19 | @ikki407 | [Multiagent Bidirectionally-Coordinated Nets for Learning to Play StarCraft Combat Games](https://arxiv.org/abs/1703.10069) | [pdf](resource/20170719-ikki407.pdf) |
|  69 | 2017/07/26 | @kiyukuta | 教科書: p.62-p.67 | [google docs](https://docs.google.com/document/d/1gcH7uqzy6SUfBtLjAkcWnuDs3yrNOgwYUeBCSX7PX88/edit) |
|  70 | 2017/08/02 | Kohno-san | [FeUdal Networks for Hierarchical Reinforcement Learning](https://arxiv.org/abs/1703.01161) | [pdf](resource/20170802-kohno.pdf) |
|  71 | 2017/08/09 | @rkawajiri | 教科書: p68-p74 | [google docs](https://docs.google.com/document/d/1pUCrvzAbnofbtK8TfDeOQv6IWP7qMVLz68Yu_2LV-18/edit) |
|  72 | 2017/08/23 | @sotetsuk | ICML2017まとめ | [google slide](https://docs.google.com/presentation/d/1SsjbN6sSXhDvq_NUqn5UX4PVJD25QVgpcpPd3W1xmVg/edit#slide=id.g245b54bc00_0_17) |
|  73 | 2017/08/30 | @YuriCat | [Learning in POMDPs with Monte Carlo Tree Search](http://proceedings.mlr.press/v70/katt17a.html) | [Dropbox paper](https://paper.dropbox.com/doc/Learning-in-POMDPs-with-Monte-Carlo-Tree-Search-6i4tvTlebl4qBDGzUH45U) |
|  74 | 2017/09/06 | @maeyon | [A Distributional Perspective on Reinforcement Learning](https://arxiv.org/abs/1707.06887) | [Dropbox paper](https://paper.dropbox.com/doc/A-Distributional-Perspective-on-Reinforcement-Learning-YSY1ciu4RmrhrRD6J5DEZ) |
|  75 | 2017/09/20 | @rkawajiri | [Improving Stochastic Policy Gradients in Continuous Control with Deep Reinforcement Learning using the Beta Distribution](http://proceedings.mlr.press/v70/chou17a.html) | [Dropbox paper](https://paper.dropbox.com/doc/Improving-Stochastic-Policy-Gradients-in-Continuous-Control-with-Deep-Reinforcement-Learning-using-the-Beta-Distribution-f1huTn8TnkQGIIgE6IZud?_tk=share_copylink) |
|  76 | 2017/10/04 | @takayukisekine | [Deep Reinforcement Learning  from Human Preference](https://arxiv.org/abs/1706.03741) | [Dropbox paper](https://paper.dropbox.com/doc/_171004_-NILSCrl6lanTqlhpqkzuP) |
|  77 | 2017/10/18 | @fullflu | [Personalized Ad Recommendation Systems for Life-Time Value Optimization with Guarantees](http://psthomas.com/papers/Theocharous2015a.pdf) | [Dropbox paper](https://paper.dropbox.com/doc/_171018-dM1cZf6vbcVIH6MARLq40) |
|  78 | 2017/11/01 | @pacocat | Poker AIの最新動向 | [pdf](https://www.slideshare.net/juneokumura/ai-20171031) |
|  79 | 2017/11/15 | @ikki407 | AlphaGo Zero | TBA |
|  80 | 2017/12/13 | Kume-san | [Uncertainty-driven Imagination for Continuous Deep Reinforcement Learning](http://proceedings.mlr.press/v78/kalweit17a/kalweit17a.pdf)/[The Intentional Unintentional Agent: Learning to Solve Many Continuous Control Tasks Simultaneously](https://arxiv.org/abs/1707.03300)| [pdf](resources/20171213-kume.pdf) |
|  81 | 2018/01/17 | @sotetsuk | NIPS2017のRL x structured (sequence) prediction系 | [Dropbox paper](https://paper.dropbox.com/doc/seq2seq-NIPS2017-YCPiFpzAGmci8qXvVBcUT) |
|  82 | 2018/01/28 | @muupan | Model-Based Reinforcement Learning @NIPS2017 | [Slideshare](https://www.slideshare.net/mooopan/modelbased-reinforcement-learning-nips2017) |
|  83 | 2018/02/14 | arakawa-kun| [Emergence of Linguistic Communication from Referential Games with Symbolic and Pixel Input](https://openreview.net/references/pdf?id=SJlP1b-0-) | [google docs](https://docs.google.com/presentation/d/1WbdhTAYdWzYQIMgE3S6g9rkxOafocgjdOaayjlUeguc/edit#slide=id.g309ca3cc62_0_125) |
|  84 | 2018/02/28 | Okumura | DQNからRainbowまで 〜深層強化学習の最新動向〜 | [Slideshare](https://www.slideshare.net/juneokumura/dqnrainbow) |
|  85 | 2018/03/14 | Ohto-kun| [Learning to Search with MCTSnets](https://arxiv.org/abs/1802.04697) | [Dropbox paper](https://paper.dropbox.com/doc/Learning-to-Search-with-MCTSnets-8QVRPzzqmO5flNzNxzg1b) |
|  86 | 2018/03/28 | Takayama-kun| [Q-learning with censored data](https://arxiv.org/abs/1205.6659) | [Dropbox paper](https://paper.dropbox.com/doc/_180328-lAuYG3I558Kop0yvTkjsf) |
|  87 | 2018/04/11 | Nishiura-kun| [Active Neural Localization](https://openreview.net/forum?id=ry6-G_66b) | [google slide](https://drive.google.com/open?id=1pHk1-dyniF1lloiiyrj2x5n3txeG0A9uY8YYgKPP90k) |
|  88 | 2018/04/25 | Kikuchi-san| [Learning an Embedding Space for Transferable Robot Skills](https://openreview.net/forum?id=rk07ZXZRb) | [Dropbox paper](https://paper.dropbox.com/doc/ICLR2018-Learning-an-Embedding-Space-for-Transferable-Robot-Skills-0iuMlK8stedQa4uDHc70H?_tk=share_copylink) |
|  89 | 2018/05/08 | @rkawajiri| [MAML](https://arxiv.org/abs/1703.03400) | [Dropbox paper](https://paper.dropbox.com/doc/Model-Agnostic-Meta-Learning-for-Fast-Adaptation-of-Deep-Networks-MAML-6M5XtdNrpeM9FzTBAiMQ4?_tk=share_copylink) |
|  90 | 2018/05/22 | Maeda-san | [Towards Symbolic Reinforcement Learning with Common Sense](https://arxiv.org/abs/1804.08597) | [Dropbox paper](https://paper.dropbox.com/doc/Towards-Symbolic-Reinforcement-Learning-with-Common-Sense-45VHtSCoe0na1RNrhHA3N) |
|  91 | 2018/06/05 | @toslunar| [Learning Deep Mean Field Games for Modeling Large Population Behavior](https://arxiv.org/pdf/1711.03156.pdf) | None |
|  92 | 2018/06/19 | Yamaguchi| [GAIL](https://arxiv.org/abs/1606.03476) | [google docs](https://docs.google.com/document/d/1j9zdjx9457Ud06RPlg91UsrSiyx1VHT7icQiqRrnFQ8/edit?usp=sharing) |
|  93 | 2018/07/03 | Arakawa-kun | [RUDDER](https://arxiv.org/abs/1806.07857) | [google slide](https://docs.google.com/presentation/d/1GOrIDkoZ_w_PpKk-nDVzLVZx4c1Ix7UTRZ99mHdKvJA/edit?usp=sharing) |
|  94 | 2018/07/17 | Ohto-kun | TBA | TBA |
|  95 | 2018/07/31 | @muupan | TBA | TBA |
|  96 | 2018/08/21 | @jun.okumura | TBA | TBA |
|  97 | 2018/09/11 | @ichi | TBA | TBA |
|  98 | 2018/10/09 | @kawajiri | TBA | TBA |
|  99 | 2018/10/23 | @maso | TBA | TBA |
|  100 | 2018/11/06 | @kume | TBA | TBA |
|  101 | 2018/11/20 | @kikuchi | TBA | TBA |
|  102 | 2018/12/04 | @takahashi | TBA | TBA |


- （注）教科書のページは "[Algorithms for Reinforcement Learning](https://sites.ualberta.ca/~szepesva/RLBook.html)" のPDFのページに対応
- （注）担当者欄はGitHubアカウントが判明している方については@つきでGitHubアカウントで掲載しています。

# お問い合わせ
[github.com/rl-tokyo/rl-tokyo.github.io/issues](https://github.com/rl-tokyo/rl-tokyo.github.io/issues/new) にて承ります。
参加者は随時募集しております。ただし持ち回りでの勉強会の発表の担当をご負担頂ける方だけになります。その点だけご了承下さい。
