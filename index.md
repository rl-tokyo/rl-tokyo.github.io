---
layout: default
---

# 活動内容

1. 毎週水曜に東京駅近郊で勉強会（教科書輪読または論文紹介）
2. その他不定期での勉強会
3. 勉強会での議論から派生したアウトプット活動（書籍・論文の出版）

1.の毎週水曜の勉強会が主な活動内容になります。

## 教科書輪読
"[Algorithms for Reinforcement Learning](https://sites.ualberta.ca/~szepesva/RLBook.html)" の輪講をしています。3週目以降は未定です。

- Szepesvári 2010 "[Algorithms for Reinforcement Learning](https://sites.ualberta.ca/~szepesva/RLBook.html)" (Morgan & Claypool)
- 毎回5ページ程度

1. 1周目: 2015/11/20〜
2. 2週目: 2016/04/06〜
3. 3週目: 2017/03/01〜

## 論文紹介

発表担当者が自分の好みで紹介したい論文を一本紹介してもらいます。
資料の作成は任意になります。
資料の作成よりは論文自体の理解を優先してもらい、
当日の発表はホワイトボード等を使って説明して頂く形でも問題ありません。
1〜2時間かけて一本をじっくり読みます。

# 活動履歴

|   # | 日付 | 担当 | 内容 | 資料 |
|:----|:-----------|:---------------|
|   1 | 2015/10/22 | @sotetsuk / @RodeoBoy24420 |キックオフ: @sotetsukから趣旨の説明と@RodeoBoy24420から強化学習概要 | [pdf](resource/20151022-RodeoBoy24420.pdf) |
|   2 | 2015/11/02 | @smochi |教科書: p.3-p.6 | |
|   3 | 2015/11/12 | @nnnnishi |教科書: p.7-p.12 | |
|   4 | 2015/11/17 | @RodeoBoy24420 |教科書: p.12-p.17 | |
|   5 | 2015/11/24 | @takayukisekine |教科書: p.17-p.23 | |
|   6 | 2015/12/02 | @ikki407 |教科書: p.23-p.29 | [pdf](resource/20151202-ikki407.pdf) |
|   7 | 2015/12/08 | @sotetsuk |教科書: p.29-p.35 | |
|   8 | 2015/12/16 | @fullflu |教科書: p.35-p.40 | [pdf](resource/20151216-fullflu.pdf) |
|   9 | 2016/01/18 | @smochi |教科書: p.40-p.45  | |
|  10 | 2016/01/27 | @nnnnishi |教科書: p.45-p.50  | |
|  11 | 2016/02/08 | @RodeoBoy24420  |教科書: p.50-p.56 | |
|  12 | 2016/02/22 | @takayukisekine |教科書: p.56-p.62 | |
|  13 | 2016/02/29 | @sotetsuk |教科書: p.62-p.67 | |
|  14 | 2016/03/28 | @ikki407  |教科書: p.68-p.74 | |
|  15 | 2016/04/06 | @sotetsuk |教科書: p.3-p.6 | |
|  16 | 2016/04/13 | @muupan |論文: [Mastering the game of Go with deep neural networks and tree search](https://vk.com/doc-44016343_437229031?dl=56ce06e325d42fbc72) | [pdf](resource/20160413-muupan.pdf) |
|  17 | 2016/04/20 | @smochi |教科書: p.7-p.12 | |
|  18 | 2016/04/27 | @takayukisekine |論文: [Deep Reinforcement Learning with Attention for Slate Markov Decision Processes with High-Dimensional States and Actions](https://arxiv.org/abs/1512.01124) | [pdf](resource/20160427-takayukisekine.pdf) |
|  19 | 2016/05/11 | @smochi |教科書: p.7-p.12（続き） | |
|  20 | 2016/05/18 | @RodeoBoy24420 |論文: [Algorithms for Inverse Reinforcement Learning](http://ai.stanford.edu/~ang/papers/icml00-irl.pdf) | [pdf](resource/20160518-RodeoBoy24420.pdf) |
|  21 | 2016/05/25 | @nnnnishi |教科書: p.12-p.17 | |
|  22 | 2016/06/01 | @sotetsuk |論文: [Dopamine Cells Respond to Predicted Events during Classical Conditioning: Evidence for Eligibility Traces in the Reward-Learning Network](http://www.jneurosci.org/content/25/26/6235) | [slideshare](https://www.slideshare.net/sotetsukoyamada/22-62639753) |
|  23 | 2016/06/08 | @YuriCat |教科書: p.17-p.23 | |
|  24 | 2016/06/15 | @ikki407 |論文: [A Contextual-Bandit Approach to Personalized News Article Recommendation](https://arxiv.org/abs/1003.0146) | [pdf](resource/20160615-ikki407.pdf) |
|  25 | 2016/06/22 | @fullflu |教科書: p.23-p.29 | |
|  26 | 2016/06/29 | Ueki-san |論文: [True Online TD(λ) ](http://proceedings.mlr.press/v32/seijen14.pdf)| |
|  27 | 2016/07/06 | @muupan |教科書: p.29-p.35 | |
|  28 | 2016/07/20 | @smochi |論文: [Dynamic pricing policies for interdependent perishable products or services using reinforcement learning](http://dl.acm.org/citation.cfm?id=2953251) | |
|  29 | 2016/08/03 | @takayukisekine |教科書: p.35-p.40 | |
|  30 | 2016/08/10 | @nnnnishi |論文: [Ensemble Contextual Bandits for Personalized Recommendation](http://dl.acm.org/citation.cfm?id=2645732) | [slideshare](https://www.slideshare.net/NaokiNISHIMURA2/30ensemble-contextual-bandits-for-personalized-recommendation) |
|  31 | 2016/08/24 | @RodeoBoy24420 |教科書: p.40-p.45 | |
|  32 | 2016/08/31 | @sotetsuk |論文: [Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation](https://arxiv.org/abs/1604.06057) | [slideshare](https://www.slideshare.net/sotetsukoyamada/kulkarni-et-al-2016) |
|  33 | 2016/09/14 | @ikki407 |教科書: p.45-p.50  | |
|  34 | 2016/09/21 | @YuriCat |論文: [Unifying Count-Based Exploration and Intrinsic Motivation](https://arxiv.org/abs/1606.01868) | [slideshare](https://www.slideshare.net/KatsukiOhto/unifying-count-based-exploration-and-intrinsic-motivation) |
|  35 | 2016/09/28 | @shiba24 |教科書: p.50-p.56 | |
|  36 | 2016/10/05 | @takayukisekine |論文: [SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient](https://arxiv.org/abs/1609.05473) | [slideshare](http://www.slideshare.net/TakayukiSekine1/seq-gan-66772221) |
|  37 | 2016/10/19 | @shiba24 |教科書: p.50-p.56 | |
|  38 | 2016/10/26 | @shiba24 |論文: [Hybrid computing using a neural network with dynamic external memory](https://www.nature.com/nature/journal/v538/n7626/abs/nature20101.html?lang=en) | [pdf](resource/20161026_dnc.pdf) |
|  39 | 2016/11/02 | @muupan |教科書: p.56-p.62 | |
|  40 | 2016/11/09 | @fullflu |論文: [DCM Bandits: Learning to Rank with Multiple Clicks](https://arxiv.org/abs/1602.03146) | [pdf](resource/20161109-fullflu.pdf) |
|  41 | 2016/11/30 | @fullflu |教科書: p.62-p.67 | |
|  43 | 2016/12/07 | @shiba24 |教科書: p.68-p.74 | |
|  44 | 2017/01/11 | @muupan |論文: [Safe and Efficient Off-PolicyReinforcement Learning](https://arxiv.org/abs/1606.02647) | [pdf](resource/20170111-muupan.pdf) |
|  45 | 2017/01/18 | @RodeoBoy24420 | 論文: [Application of fuzzy Q-learning for electricity market modeling by considering renewable power penetration](http://www.sciencedirect.com/science/article/pii/S1364032115014033) | [pdf](resource/20170118-RodeoBoy24420.pdf) |
|  46 | 2017/02/01 | @STRatANG |論文: [Sample Efficient Actor-Critic with Experience Replay](https://arxiv.org/abs/1611.01224) | [pdf](resource/20170201-STRatANG.pdf) |
|  47 | 2017/02/08 | @ororoku |論文: [The Dynamics of Reinforcement Learning in Cooperative Multiagent Systems](https://www.aaai.org/Papers/AAAI/1998/AAAI98-106.pdf) | [pdf](resource/20170208-ororoku.pdf) |
|  48 | 2017/02/15 | @smochi |論文: [Artist Agent: A Reinforcement Learning Approach to Automatic Stroke Generation in Oriental Ink Painting](https://arxiv.org/abs/1206.4634) | [pdf](resource/20170212-smochi.pdf) |
|  49 | 2017/03/01 | @sotetsuk | 教科書: p.3-p.12 | |
|  50 | 2017/03/08 | @nnnnishi |論文: [Optimal Asset Allocation using Adaptive Dynamic Programming](https://papers.nips.cc/paper/1121-optimal-asset-allocation-using-adaptive-dynamic-programming.pdf) / [Enhancing Q-Learning for Optimal Asset Allocation](https://papers.nips.cc/paper/1427-enhancing-q-learning-for-optimal-asset-allocation.pdf) | [pdf](resource/20170308-nnnnishi.pdf) |
|  51 | 2017/03/15 | @STRatANG | 教科書: p.12-p.17 | |
|  52 | 2017/03/22 | @YuriCat |論文: [Combining policy gradient and Q-learning](https://arxiv.org/abs/1611.01626) | [slideshare](https://www.slideshare.net/sotetsukoyamada/pgq-combining-policy-gradient-and-qlearning) |
|  53 | 2017/03/29 | @pacocat | 教科書: p.17-p.23 | |
|  54 | 2017/04/05 | @takayukisekine |論文: [Evolution Strategies as a Scalable Alternative to Reinforcement Learning](https://arxiv.org/abs/1703.03864) | [pdf](resource/20170405-sekine.pdf) |
|  55 | 2017/04/12 | @kiyukuta | 教科書: p.23-p.29 | |
|  56 | 2017/04/19 | @sotetsuk |論文: [Reward Augmented Maximum Likelihood for Neural Structured Prediction](https://arxiv.org/abs/1609.00150) | [slideshare](https://www.slideshare.net/sotetsukoyamada/reward-augmented-maximum-likelihood-for-neural-structured-prediction) |
|  57 | 2017/04/26 | Kume-san | 教科書: p.29-p.35 | |
|  58 | 2017/05/10 | Yamada-san |論文: [Overcoming catastrophic forgetting in neural networks](https://arxiv.org/abs/1612.00796) | [pdf](resource/20170510-yamada.pdf) |
|  59 | 2017/05/17 | @ororoku | 教科書: p.35-p.40 | |
|  60 | 2017/05/24 | @smochi |論文: [Prioritized Experience Replay](https://arxiv.org/abs/1511.05952) | [google slide](https://docs.google.com/presentation/d/1P_ks8cqXcQmc8rBk7QlxcBHwfSdlNYnPmnWF0yj_nYs/edit#slide=id.g22de88c68b_1_160) |
|  61 | 2017/05/31 | @eratostennis | 教科書: p.40-p.45 | |
|  62 | 2017/06/07 | @sotetsuk |論文: [Bridging the Gap Between Value and Policy Based Reinforcement Learning](https://arxiv.org/abs/1702.08892) | [Dropbox paper](https://paper.dropbox.com/doc/Bridging-the-Gap-Between-Value-and-Policy-Based-Reinforcement-Learning-JmxgdnSMzEQUmBTk1C31T) |
|  63 | 2017/06/14 | @STRatANG | 教科書: p.45-p.50 | [当日メモ](https://paper.dropbox.com/doc/20170614-63-CKfWM2TQ0gyQW4aGEypZ0) |
|  64 | 2017/06/21 | @YuriCat | [The Predictron: End-To-End Learning and Planning](https://arxiv.org/abs/1612.08810) | [Dropbox paper](https://paper.dropbox.com/doc/The-Predictron-End-to-End-Learning-and-Planning-B8bAPaUfElK0yhEPP2Qgu) |
|  65 | 2017/06/28 | @fullflu | 教科書: p.50-p.56 | [当日メモ](https://paper.dropbox.com/doc/65-i42HBMyaDfTc3wCEasm1c) |
|  66 | 2017/07/05 | @shiba24 | [Trust Region Policy Optimization](https://arxiv.org/abs/1502.05477) | [google slide](https://docs.google.com/presentation/d/1BUAbeNbO_-cEBpFQgN2mnrSaUeF-QLD9OPIFpai9Gz8/edit) / [当日メモ](https://paper.dropbox.com/doc/66-dRD0JX8C0PTnsMi8E1vaF) |
|  67 | 2017/07/12 | Kume-san | 教科書: p.56-p.62 | [当日メモ](https://paper.dropbox.com/doc/67-d8PCnv47TrSSa1tNqibhN) |
|  68 | 2017/07/19 | @ikki407 | [Multiagent Bidirectionally-Coordinated Nets for Learning to Play StarCraft Combat Games](https://arxiv.org/abs/1703.10069) | [pdf](resource/20170719-ikki407.pdf) |
|  69 | 2017/07/26 | @kiyukuta | 教科書: p.62-p.67 | [google docs](https://docs.google.com/document/d/1gcH7uqzy6SUfBtLjAkcWnuDs3yrNOgwYUeBCSX7PX88/edit) |
|  70 | 2017/08/02 | Kohno-san | [FeUdal Networks for Hierarchical Reinforcement Learning](https://arxiv.org/abs/1703.01161) | [pdf](resource/20170802-kohno.pdf) |
|  71 | 2017/08/09 | @rkawajiri | 教科書: p68-p74 | [google docs](https://docs.google.com/document/d/1pUCrvzAbnofbtK8TfDeOQv6IWP7qMVLz68Yu_2LV-18/edit) |
|  72 | 2017/08/23 | @sotetsuk | ICML2017まとめ | [google slide](https://docs.google.com/presentation/d/1SsjbN6sSXhDvq_NUqn5UX4PVJD25QVgpcpPd3W1xmVg/edit#slide=id.g245b54bc00_0_17) |
|  73 | 2017/08/30 | @YuriCat | [Learning in POMDPs with Monte Carlo Tree Search](http://proceedings.mlr.press/v70/katt17a.html) | [Dropbox paper](https://paper.dropbox.com/doc/Learning-in-POMDPs-with-Monte-Carlo-Tree-Search-6i4tvTlebl4qBDGzUH45U) |
|  74 | 2017/09/06 | @maeyon | [A Distributional Perspective on Reinforcement Learning](https://arxiv.org/abs/1707.06887) | [Dropbox paper](https://paper.dropbox.com/doc/A-Distributional-Perspective-on-Reinforcement-Learning-YSY1ciu4RmrhrRD6J5DEZ) |
|  75 | 2017/09/20 | @rkawajiri | [Improving Stochastic Policy Gradients in Continuous Control with Deep Reinforcement Learning using the Beta Distribution](http://proceedings.mlr.press/v70/chou17a.html) | [Dropbox paper](https://paper.dropbox.com/doc/Improving-Stochastic-Policy-Gradients-in-Continuous-Control-with-Deep-Reinforcement-Learning-using-the-Beta-Distribution-f1huTn8TnkQGIIgE6IZud?_tk=share_copylink) |
|  76 | 2017/10/04 | @takayukisekine | [Deep Reinforcement Learning  from Human Preference](https://arxiv.org/abs/1706.03741) | [Dropbox paper](https://paper.dropbox.com/doc/_171004_-NILSCrl6lanTqlhpqkzuP) |
|  77 | 2017/10/18 | @fullflu | [Personalized Ad Recommendation Systems for Life-Time Value Optimization with Guarantees](http://psthomas.com/papers/Theocharous2015a.pdf) | [Dropbox paper](https://paper.dropbox.com/doc/_171018-dM1cZf6vbcVIH6MARLq40) |
|  78 | 2017/11/01 | @pacocat | ポーカーAI |  |
|  79 | 2017/11/15 | @ikki407 | AlphaGo Zero |  |

- （注）教科書のページは "[Algorithms for Reinforcement Learning](https://sites.ualberta.ca/~szepesva/RLBook.html)" のPDFのページに対応
- （注）担当者欄はGitHubアカウントが判明している方については@つきでGitHubアカウントで掲載しています。

# お問い合わせ
[github.com/rl-tokyo/rl-tokyo.github.io/issues](https://github.com/rl-tokyo/rl-tokyo.github.io/issues/new) にて承ります。
参加者は随時募集しております。ただし持ち回りでの勉強会の発表の担当をご負担頂ける方だけになります。その点だけご了承下さい。
